{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915ca393",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:27.978211Z",
     "iopub.status.busy": "2023-03-14T23:31:27.977831Z",
     "iopub.status.idle": "2023-03-14T23:31:37.531780Z",
     "shell.execute_reply": "2023-03-14T23:31:37.530635Z"
    },
    "papermill": {
     "duration": 9.564079,
     "end_time": "2023-03-14T23:31:37.534627",
     "exception": false,
     "start_time": "2023-03-14T23:31:27.970548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import errno\n",
    "from torch.cuda.amp import autocast\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "import psutil\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7e873e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.549066Z",
     "iopub.status.busy": "2023-03-14T23:31:37.546893Z",
     "iopub.status.idle": "2023-03-14T23:31:37.558935Z",
     "shell.execute_reply": "2023-03-14T23:31:37.558015Z"
    },
    "papermill": {
     "duration": 0.020822,
     "end_time": "2023-03-14T23:31:37.561081",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.540259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Configuration:\n",
    "    \n",
    "    # Transformer\n",
    "    transformer =  (\n",
    "                        '/kaggle/input/transformer-offline-no-weights/LaBSE',\n",
    "                        '/kaggle/input/transformer-offline-no-weights/mcontriever-msmarco',\n",
    "                        '/kaggle/input/transformer-offline-no-weights/paraphrase-multilingual-mpnet-base-v2',\n",
    "                        '/kaggle/input/transformer-offline-no-weights/stsb-xlm-r-multilingual',\n",
    "                        '/kaggle/input/transformer-offline-no-weights/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
    "                    )\n",
    "    \n",
    "    # Weights\n",
    "    checkpoints =  (\n",
    "                        '/kaggle/input/labse-gpu/weights_end.pth',\n",
    "                        '/kaggle/input/mcontriever-msmarco-gpu/weights_end.pth',\n",
    "                        '/kaggle/input/mpnet-gpu/weights_end.pth',\n",
    "                        '/kaggle/input/stsb-xlm-r-gpu/weights_end.pth',\n",
    "                        '/kaggle/input/xlm-r-100langs-gpu/weights_end.pth'\n",
    "                    )\n",
    "    \n",
    "    # Known content IDs\n",
    "    content_ids = (\n",
    "                       '/kaggle/input/labse-gpu/content_ids.pt', \n",
    "                       '/kaggle/input/mcontriever-msmarco-gpu/content_ids.pt',\n",
    "                       '/kaggle/input/mpnet-gpu/content_ids.pt',\n",
    "                       '/kaggle/input/stsb-xlm-r-gpu/content_ids.pt',\n",
    "                       '/kaggle/input/xlm-r-100langs-gpu/content_ids.pt'\n",
    "                  )\n",
    "    \n",
    "    # Known conten features\n",
    "    content_features =  (\n",
    "                         '/kaggle/input/labse-gpu/content_features.pt', \n",
    "                         '/kaggle/input/mcontriever-msmarco-gpu/content_features.pt',\n",
    "                         '/kaggle/input/mpnet-gpu/content_features.pt',\n",
    "                         '/kaggle/input/stsb-xlm-r-gpu/content_features.pt',\n",
    "                         '/kaggle/input/xlm-r-100langs-gpu/content_features.pt'\n",
    "                        ) \n",
    "    \n",
    "    # Known content language\n",
    "    content_language =  (\n",
    "                         '/kaggle/input/labse-gpu/content_language.pt', \n",
    "                         '/kaggle/input/mcontriever-msmarco-gpu/content_language.pt',\n",
    "                         '/kaggle/input/mpnet-gpu/content_language.pt',\n",
    "                         '/kaggle/input/stsb-xlm-r-gpu/content_language.pt',\n",
    "                         '/kaggle/input/xlm-r-100langs-gpu/content_language.pt'\n",
    "                        )\n",
    "\n",
    "    \n",
    "    # Predict \n",
    "    max_len: int = 96             # max len of tokenized topic and content\n",
    "    batch_size: int = 64          # batch size (keep small for max performance/speed)\n",
    "    margin: float = 0.18          # dynamic threshold margin  \n",
    "        \n",
    "    # set num_workers\n",
    "    num_workers: int = psutil.cpu_count(logical=False)  # CPU Cores\n",
    "    \n",
    "    # use GPU \n",
    "    device: str = 'cuda'\n",
    "    gpu_ids: int = (0,)            # (0,1) if T4\n",
    "            \n",
    "    # Testing\n",
    "    verbose: bool     = False      # show progress bar\n",
    "    speed_test: bool  = False      # encode 10000 known contents for speed testing\n",
    "    input_test: bool  = False      # check if all ids are aligned of known content\n",
    "    output_test: bool = False      # use 5000 topics instead of sample submission and evaluate \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c39bbc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.572251Z",
     "iopub.status.busy": "2023-03-14T23:31:37.571973Z",
     "iopub.status.idle": "2023-03-14T23:31:37.577418Z",
     "shell.execute_reply": "2023-03-14T23:31:37.576509Z"
    },
    "papermill": {
     "duration": 0.013548,
     "end_time": "2023-03-14T23:31:37.579478",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.565930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Configuration() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca77e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.590687Z",
     "iopub.status.busy": "2023-03-14T23:31:37.590069Z",
     "iopub.status.idle": "2023-03-14T23:31:37.657492Z",
     "shell.execute_reply": "2023-03-14T23:31:37.656625Z"
    },
    "papermill": {
     "duration": 0.075668,
     "end_time": "2023-03-14T23:31:37.659935",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.584267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 transformer_name,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(transformer_name)\n",
    "        print(self.config)\n",
    "        self.transformer = AutoModel.from_config(config=self.config)  \n",
    "        \n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "                \n",
    "    @autocast()\n",
    "    def forward(self, ids, mask): \n",
    "\n",
    "        transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "            \n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c0aac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.670857Z",
     "iopub.status.busy": "2023-03-14T23:31:37.670586Z",
     "iopub.status.idle": "2023-03-14T23:31:37.681126Z",
     "shell.execute_reply": "2023-03-14T23:31:37.680071Z"
    },
    "papermill": {
     "duration": 0.018591,
     "end_time": "2023-03-14T23:31:37.683361",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.664770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EqualDatasetEval(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 text_list,\n",
    "                 ids_list,\n",
    "                 language_list,\n",
    "                 tokenizer,\n",
    "                 max_len):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.text_list = text_list\n",
    "        self.ids_list = ids_list\n",
    "        self.language_list = language_list\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text_id = self.ids_list[index]\n",
    "        language = self.language_list[index]\n",
    "        \n",
    "        tok = self.tokenizer.encode_plus(self.text_list[index],\n",
    "                                        None,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=self.max_len,\n",
    "                                        padding=\"max_length\",\n",
    "                                        return_token_type_ids=True,\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "        \n",
    "        return tok['input_ids'], tok['attention_mask'], text_id, language\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids_list)\n",
    "            \n",
    "\n",
    "    def smart_batching_collate(self, batch):\n",
    "        \n",
    "        input_ids = [x[0] for x in batch]\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        \n",
    "        mask = [x[1] for x in batch]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "        \n",
    "        max_seq_length = mask.sum(-1).max().to(torch.long)\n",
    "        \n",
    "        # smart cutoff\n",
    "        input_ids = input_ids[:, :max_seq_length]\n",
    "        mask = mask[:, :max_seq_length]\n",
    "        \n",
    "        text_id =  [x[2] for x in batch]\n",
    "        \n",
    "        language = [x[3] for x in batch]\n",
    "        \n",
    "        return input_ids, mask, text_id, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c347081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.695578Z",
     "iopub.status.busy": "2023-03-14T23:31:37.693946Z",
     "iopub.status.idle": "2023-03-14T23:31:37.703098Z",
     "shell.execute_reply": "2023-03-14T23:31:37.702106Z"
    },
    "papermill": {
     "duration": 0.017126,
     "end_time": "2023-03-14T23:31:37.705234",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.688108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(config, model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if config.verbose:\n",
    "        bar = tqdm(dataloader, total=len(dataloader))\n",
    "    else:\n",
    "        bar = dataloader\n",
    "        \n",
    "    features_list = []\n",
    "    ids_list = []\n",
    "    language_list = []\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for ids, mask, text_id, language_id in bar:\n",
    "            \n",
    "            ids_list.extend(text_id)\n",
    "            language_list.extend(language_id)\n",
    "        \n",
    "            with autocast():\n",
    "\n",
    "                ids = ids.to(config.device)\n",
    "                mask = mask.to(config.device)\n",
    "        \n",
    "                feature = model(ids, mask)\n",
    "                feature = F.normalize(feature, dim=-1)\n",
    "            \n",
    "            # normalize output is fp32 with autocast\n",
    "            features_list.append(feature.to(torch.float16))\n",
    "\n",
    "        features = torch.cat(features_list, dim=0).to(\"cpu\")\n",
    "        \n",
    "    if config.verbose:\n",
    "        bar.close()\n",
    "        \n",
    "    ids_list = np.array(ids_list)\n",
    "    language_list = np.array(language_list)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(f\"Time for feature extraction: {t1-t0:.3f} sec\")    \n",
    "        \n",
    "       \n",
    "    return features, ids_list, language_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b4cd79f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.716440Z",
     "iopub.status.busy": "2023-03-14T23:31:37.715652Z",
     "iopub.status.idle": "2023-03-14T23:31:37.722304Z",
     "shell.execute_reply": "2023-03-14T23:31:37.721334Z"
    },
    "papermill": {
     "duration": 0.014559,
     "end_time": "2023-03-14T23:31:37.724558",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.709999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    x = str(x)\n",
    "    if x != \"\" and len(x) > 1:\n",
    "        x = x.strip().strip('\\t').strip('\\n')\n",
    "    return x\n",
    "    \n",
    "def clean_and_cut(x):\n",
    "    x = str(x)\n",
    "    if x != \"\" and len(x) > 1:\n",
    "        x = x.strip().strip('\\t').strip('\\n').replace(\"\", \"\")\n",
    "        x = re.sub(r'http\\S+', '', x)\n",
    "        x = \" \".join(x.split(\" \")[:32])[:256]      \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f70218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:37.735429Z",
     "iopub.status.busy": "2023-03-14T23:31:37.735077Z",
     "iopub.status.idle": "2023-03-14T23:31:39.721660Z",
     "shell.execute_reply": "2023-03-14T23:31:39.720572Z"
    },
    "papermill": {
     "duration": 1.994794,
     "end_time": "2023-03-14T23:31:39.724182",
     "exception": false,
     "start_time": "2023-03-14T23:31:37.729388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\", index_col=0)\n",
    "df_topics = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/topics.csv\", index_col=0).fillna({\"title\": \"\", \"description\": \"\"})\n",
    "\n",
    "topic2language = dict(zip(df_topics.index, df_topics[\"language\"]))\n",
    "\n",
    "df_sample_submission[\"language\"] = df_sample_submission.index.map(lambda x : topic2language.get(x, \"unk\"))\n",
    "\n",
    "df_topics = df_topics.replace(to_replace= r'\\r\\n', value= ' ', regex=True)\n",
    "df_topics = df_topics.replace(to_replace= r'\\n', value= ' ', regex=True)\n",
    "\n",
    "df_topics[\"title\"] = df_topics[\"title\"].map(clean)\n",
    "df_topics[\"description\"] = df_topics[\"description\"].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea72f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:39.737033Z",
     "iopub.status.busy": "2023-03-14T23:31:39.735998Z",
     "iopub.status.idle": "2023-03-14T23:31:39.747267Z",
     "shell.execute_reply": "2023-03-14T23:31:39.746182Z"
    },
    "papermill": {
     "duration": 0.01993,
     "end_time": "2023-03-14T23:31:39.749405",
     "exception": false,
     "start_time": "2023-03-14T23:31:39.729475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, topic_id):\n",
    "        self.id = topic_id\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def parent(self):\n",
    "        parent_id = df_topics.loc[self.id].parent\n",
    "        if pd.isna(parent_id):\n",
    "            return None\n",
    "        else:\n",
    "            return Topic(parent_id)\n",
    "\n",
    "    @property\n",
    "    def ancestors(self):\n",
    "        ancestors = []\n",
    "        parent = self.parent\n",
    "        while parent is not None:\n",
    "            ancestors.append(parent)\n",
    "            parent = parent.parent\n",
    "        return ancestors\n",
    "\n",
    "    @property\n",
    "    def siblings(self):\n",
    "        if not self.parent:\n",
    "            return []\n",
    "        else:\n",
    "            return [topic for topic in self.parent.children if topic != self]\n",
    "\n",
    "    def get_breadcrumbs(self, separator=\" >> \", include_self=True, include_root=True):\n",
    "        ancestors = self.ancestors\n",
    "        if include_self:\n",
    "            ancestors = [self] + ancestors\n",
    "        if not include_root:\n",
    "            ancestors = ancestors[:-1]\n",
    "        #return separator.join(reversed([a.title for a in ancestors]))\n",
    "        return separator.join([a.title for a in ancestors])\n",
    "\n",
    "    @property\n",
    "    def children(self):\n",
    "        return [Topic(child_id) for child_id in df_topics[df_topics.parent == self.id].index]\n",
    "\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Topic):\n",
    "            return False\n",
    "        return self.id == other.id\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return df_topics.loc[self.id][name]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.title\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<Topic(id={self.id}, title=\\\"{self.title}\\\")>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdc3b74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:39.760680Z",
     "iopub.status.busy": "2023-03-14T23:31:39.760363Z",
     "iopub.status.idle": "2023-03-14T23:31:39.977617Z",
     "shell.execute_reply": "2023-03-14T23:31:39.976558Z"
    },
    "papermill": {
     "duration": 0.226563,
     "end_time": "2023-03-14T23:31:39.980853",
     "exception": false,
     "start_time": "2023-03-14T23:31:39.754290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.output_test:\n",
    "    df_correlations = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv\", index_col=0)\n",
    "    topic_test = df_correlations.index.values[:5000]\n",
    "    df_correlations[\"language\"] = df_correlations.index.map(lambda x : topic2language.get(x, \"unk\"))\n",
    "    language_t = df_correlations[\"language\"].values[:5000].tolist()\n",
    "    del df_correlations\n",
    "else:\n",
    "    topic_test = df_sample_submission.index.values \n",
    "    language_t = df_sample_submission[\"language\"].values.tolist()\n",
    "     \n",
    "def get_topic2string(topic_test): \n",
    "    \n",
    "    topic2string = {}\n",
    "\n",
    "    if config.verbose:\n",
    "        bar = tqdm(topic_test, total=len(topic_test))\n",
    "    else:\n",
    "        bar = topic_test\n",
    "\n",
    "    for t in bar:\n",
    "        to = Topic(t)\n",
    "        string = \"{} # {}\".format(to.get_breadcrumbs(separator=\" # \", include_self=True), to.description)\n",
    "        topic2string[t] = string\n",
    "        \n",
    "    return topic2string\n",
    "\n",
    "\n",
    "topic2string = get_topic2string(topic_test)\n",
    "\n",
    "ids_t = topic_test.tolist()\n",
    "\n",
    "text_t = []\n",
    "for t in ids_t:\n",
    "    text_t.append(topic2string[t])\n",
    "    \n",
    "    \n",
    "del df_topics, df_sample_submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50da9085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:39.992445Z",
     "iopub.status.busy": "2023-03-14T23:31:39.992128Z",
     "iopub.status.idle": "2023-03-14T23:31:39.999696Z",
     "shell.execute_reply": "2023-03-14T23:31:39.998590Z"
    },
    "papermill": {
     "duration": 0.016119,
     "end_time": "2023-03-14T23:31:40.002160",
     "exception": false,
     "start_time": "2023-03-14T23:31:39.986041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.input_test:\n",
    "    \n",
    "    # Check if all pre-extracted features have same order\n",
    "    content_ids_0 = np.array(torch.load(config.content_ids[0]))\n",
    "    content_language_0 = np.array(torch.load(config.content_language[0]))\n",
    "\n",
    "    for i in range(1,len(config.transformer)):\n",
    "\n",
    "        print(\"-\"*30, i, \"-\"*30)\n",
    "\n",
    "        content_ids = np.array(torch.load(config.content_ids[i]))\n",
    "        content_language = np.array(torch.load(config.content_language[i]))\n",
    "        content_features = torch.load(config.content_features[i])\n",
    "\n",
    "        if (content_ids_0 == content_ids).sum() == len(content_ids):\n",
    "            print(\"Sucess: Same order of Content and Content\")\n",
    "        else:\n",
    "            print(\"Error: Content and Conten have not the same order!!!\")\n",
    "\n",
    "        if (content_language_0 == content_language).sum() == len(content_language_0):\n",
    "            print(\"Sucess: Same order of Content Language and Content Language\")\n",
    "        else:\n",
    "            print(\"Error: Content Language and Content Language have not the same order!!!\")\n",
    "            \n",
    "    del content_ids_0, content_language_0, content_features, content_ids\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "620e96af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:40.013263Z",
     "iopub.status.busy": "2023-03-14T23:31:40.012981Z",
     "iopub.status.idle": "2023-03-14T23:31:59.730621Z",
     "shell.execute_reply": "2023-03-14T23:31:59.729331Z"
    },
    "papermill": {
     "duration": 19.725892,
     "end_time": "2023-03-14T23:31:59.732925",
     "exception": false,
     "start_time": "2023-03-14T23:31:40.007033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght all content:     154047\n",
      "Lenght known content:   154047\n",
      "Lenght unknown content: 0\n"
     ]
    }
   ],
   "source": [
    "df_content = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/content.csv\", index_col=0).fillna({\"title\": \"\", \"description\": \"\", \"text\": \"\"})\n",
    "\n",
    "content_ids = np.array(torch.load(config.content_ids[0]))\n",
    "content_language = np.array(torch.load(config.content_language[0]))\n",
    "\n",
    "# For speed Test on Content\n",
    "if config.speed_test:\n",
    "    known_content = set(content_ids[:-10000])\n",
    "else:\n",
    "    known_content = set(content_ids)\n",
    "      \n",
    "df_content[\"known\"] = df_content.index.map(lambda x : x in known_content)\n",
    "\n",
    "unknown_content_df = df_content[df_content[\"known\"] == False].copy()\n",
    "known_content_df = df_content[df_content[\"known\"] == True]\n",
    "\n",
    "print(\"Lenght all content:    \", len(df_content))\n",
    "print(\"Lenght known content:  \", len(known_content_df))\n",
    "print(\"Lenght unknown content:\", len(unknown_content_df))\n",
    "\n",
    "known_content_ids =  set(known_content_df.index)\n",
    "\n",
    "known_content_selector = []\n",
    "\n",
    "for i, c in enumerate(content_ids):\n",
    "\n",
    "    if c in known_content_ids:\n",
    "        known_content_selector.append(True)\n",
    "    else:\n",
    "        known_content_selector.append(False)\n",
    "        \n",
    "known_content_selector = np.array(known_content_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d567798b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:31:59.745409Z",
     "iopub.status.busy": "2023-03-14T23:31:59.744555Z",
     "iopub.status.idle": "2023-03-14T23:31:59.995542Z",
     "shell.execute_reply": "2023-03-14T23:31:59.994407Z"
    },
    "papermill": {
     "duration": 0.261338,
     "end_time": "2023-03-14T23:31:59.999863",
     "exception": false,
     "start_time": "2023-03-14T23:31:59.738525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of unknown content to process: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New Content\n",
    "unknown_content_df = unknown_content_df.replace(to_replace= r'\\r\\n', value= ' ', regex=True)\n",
    "unknown_content_df = unknown_content_df.replace(to_replace= r'\\n', value= ' ', regex=True)\n",
    "\n",
    "unknown_content_df[\"title\"] = unknown_content_df[\"title\"].map(clean)\n",
    "unknown_content_df[\"description\"] = unknown_content_df[\"description\"].map(clean)\n",
    "unknown_content_df[\"text\"] = unknown_content_df[\"text\"].map(clean_and_cut)\n",
    "\n",
    "unknown_content_df[\"text_cut\"] = unknown_content_df[\"text\"].map(lambda x : \" \".join(x.split(\" \")[:32]))\n",
    "unknown_content_df[\"input\"] = unknown_content_df[\"title\"] + \" # \" + unknown_content_df[\"description\"] + \" # \" +  unknown_content_df[\"text_cut\"]\n",
    "\n",
    "text_c = unknown_content_df[\"input\"].values.tolist()\n",
    "ids_c = unknown_content_df.index.tolist()\n",
    "language_c = unknown_content_df[\"language\"].values.tolist()\n",
    "\n",
    "print(f\"Lenght of unknown content to process: {len(text_c)}\")\n",
    "\n",
    "del df_content, unknown_content_df, known_content_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9846e5bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:32:00.012848Z",
     "iopub.status.busy": "2023-03-14T23:32:00.012557Z",
     "iopub.status.idle": "2023-03-14T23:32:00.025611Z",
     "shell.execute_reply": "2023-03-14T23:32:00.024498Z"
    },
    "papermill": {
     "duration": 0.022594,
     "end_time": "2023-03-14T23:32:00.027781",
     "exception": false,
     "start_time": "2023-03-14T23:32:00.005187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features_model(config, text_t, ids_t, language_t, text_c, ids_c, language_c, known_content_selector):\n",
    "    \n",
    "    \n",
    "    for i, t in enumerate(config.transformer):\n",
    "\n",
    "        print(\"\\n{}[Model: {}]{}\".format(20*\"-\", t, 20*\"-\"))\n",
    "\n",
    "        model = Net(transformer_name=t).eval().to(torch.device(config.device))\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(t)\n",
    "    \n",
    "        print(\"Loading Checkpoint:\", config.checkpoints[i])\n",
    "        model.load_state_dict(torch.load(config.checkpoints[i], map_location=torch.device(config.device)), strict=True)\n",
    "        \n",
    "        # Eval\n",
    "        val_dataset_topic = EqualDatasetEval(text_list=text_t,\n",
    "                                             ids_list=ids_t,\n",
    "                                             language_list=language_t,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             max_len=config.max_len)\n",
    "\n",
    "        val_loader_topic = DataLoader(dataset=val_dataset_topic, \n",
    "                                      batch_size=config.batch_size, \n",
    "                                      shuffle=False,\n",
    "                                      num_workers=config.num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      collate_fn=val_dataset_topic.smart_batching_collate\n",
    "                                      )\n",
    "        \n",
    "        \n",
    "        topic_features, topic_ids, topic_language = predict(config, model, val_loader_topic)\n",
    "        \n",
    "        torch.save(topic_features, f\"topic_features_{i}.pt\")\n",
    "        \n",
    "        content_ids = np.array(torch.load(config.content_ids[i]))[known_content_selector]\n",
    "        content_language = np.array(torch.load(config.content_language[i]))[known_content_selector]\n",
    "        content_features = torch.load(config.content_features[i])[known_content_selector]\n",
    "        \n",
    "        # if new content update content\n",
    "        if len(text_c) > 0:\n",
    "        \n",
    "\n",
    "            val_dataset_content = EqualDatasetEval(text_list=text_c,\n",
    "                                                   ids_list=ids_c,\n",
    "                                                   language_list=language_c,\n",
    "                                                   tokenizer=tokenizer,\n",
    "                                                   max_len=config.max_len)\n",
    "\n",
    "            val_loader_content = DataLoader(dataset=val_dataset_content, \n",
    "                                            batch_size=config.batch_size, \n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config.num_workers,\n",
    "                                            pin_memory=True,\n",
    "                                            collate_fn=val_dataset_content.smart_batching_collate\n",
    "                                            )\n",
    "\n",
    "       \n",
    "            content_features_new, content_ids_new, content_language_new = predict(config, model, val_loader_content) \n",
    "        \n",
    "            # Add new content\n",
    "            content_ids = np.concatenate([content_ids, content_ids_new])\n",
    "            content_language = np.concatenate([content_language, content_language_new])\n",
    "            content_features = torch.cat([content_features, content_features_new], dim=0)\n",
    "        \n",
    "\n",
    "        torch.save(content_features, f\"content_features_{i}.pt\")\n",
    "                \n",
    "        if i == 0:        \n",
    "            # Save only once cause we checked that for all models the same order\n",
    "            torch.save(topic_ids, f\"topic_ids.pt\")\n",
    "            torch.save(topic_language, f\"topic_language.pt\")\n",
    "\n",
    "            torch.save(content_ids, f\"content_ids.pt\")\n",
    "            torch.save(content_language, f\"content_language.pt\")\n",
    "    \n",
    "        del model, content_features, topic_features\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bac57e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:32:00.039967Z",
     "iopub.status.busy": "2023-03-14T23:32:00.039004Z",
     "iopub.status.idle": "2023-03-14T23:33:34.096615Z",
     "shell.execute_reply": "2023-03-14T23:33:34.095375Z"
    },
    "papermill": {
     "duration": 94.066234,
     "end_time": "2023-03-14T23:33:34.099067",
     "exception": false,
     "start_time": "2023-03-14T23:32:00.032833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------[Model: /kaggle/input/transformer-offline-no-weights/LaBSE]--------------------\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"/kaggle/input/transformer-offline-no-weights/LaBSE\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 501153\n",
      "}\n",
      "\n",
      "Loading Checkpoint: /kaggle/input/labse-gpu/weights_end.pth\n",
      "Time for feature extraction: 1.980 sec\n",
      "\n",
      "--------------------[Model: /kaggle/input/transformer-offline-no-weights/mcontriever-msmarco]--------------------\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"/kaggle/input/transformer-offline-no-weights/mcontriever-msmarco\",\n",
      "  \"architectures\": [\n",
      "    \"Contriever\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pooling\": \"average\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "Loading Checkpoint: /kaggle/input/mcontriever-msmarco-gpu/weights_end.pth\n",
      "Time for feature extraction: 0.292 sec\n",
      "\n",
      "--------------------[Model: /kaggle/input/transformer-offline-no-weights/paraphrase-multilingual-mpnet-base-v2]--------------------\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/kaggle/input/transformer-offline-no-weights/paraphrase-multilingual-mpnet-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "Loading Checkpoint: /kaggle/input/mpnet-gpu/weights_end.pth\n",
      "Time for feature extraction: 0.123 sec\n",
      "\n",
      "--------------------[Model: /kaggle/input/transformer-offline-no-weights/stsb-xlm-r-multilingual]--------------------\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/kaggle/input/transformer-offline-no-weights/stsb-xlm-r-multilingual\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "Loading Checkpoint: /kaggle/input/stsb-xlm-r-gpu/weights_end.pth\n",
      "Time for feature extraction: 0.113 sec\n",
      "\n",
      "--------------------[Model: /kaggle/input/transformer-offline-no-weights/xlm-r-100langs-bert-base-nli-stsb-mean-tokens]--------------------\n",
      "XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/kaggle/input/transformer-offline-no-weights/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "Loading Checkpoint: /kaggle/input/xlm-r-100langs-gpu/weights_end.pth\n",
      "Time for feature extraction: 0.117 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features_model(config, text_t, ids_t, language_t, text_c, ids_c, language_c, known_content_selector)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec6df563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:33:34.113234Z",
     "iopub.status.busy": "2023-03-14T23:33:34.112915Z",
     "iopub.status.idle": "2023-03-14T23:33:35.613226Z",
     "shell.execute_reply": "2023-03-14T23:33:35.611523Z"
    },
    "papermill": {
     "duration": 1.510883,
     "end_time": "2023-03-14T23:33:35.616306",
     "exception": false,
     "start_time": "2023-03-14T23:33:34.105423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: ['bg', 'en', 'pt']\n",
      "\n",
      "Calculate Similiarity for: /kaggle/input/transformer-offline-no-weights/LaBSE\n",
      "\n",
      "Calculate Similiarity for: /kaggle/input/transformer-offline-no-weights/mcontriever-msmarco\n",
      "\n",
      "Calculate Similiarity for: /kaggle/input/transformer-offline-no-weights/paraphrase-multilingual-mpnet-base-v2\n",
      "\n",
      "Calculate Similiarity for: /kaggle/input/transformer-offline-no-weights/stsb-xlm-r-multilingual\n",
      "\n",
      "Calculate Similiarity for: /kaggle/input/transformer-offline-no-weights/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\n",
      "Calculate mean of similiarities of 5 models per language\n",
      "\n",
      "Select content per language using dynamic threshold of 0.18:\n",
      "bg  - (2x6050) - selected: 4\n",
      "en  - (2x65939) - selected: 1\n",
      "pt  - (1x10435) - selected: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_ids =   torch.load(\"topic_ids.pt\")\n",
    "topic_language =  torch.load(\"topic_language.pt\")\n",
    "\n",
    "content_ids = torch.load(\"content_ids.pt\")\n",
    "content_language =  torch.load(\"content_language.pt\")\n",
    "\n",
    "language_count = Counter(topic_language).most_common()\n",
    "language_list = [l for l, _ in language_count]\n",
    "\n",
    "print(\"Languages:\", language_list)\n",
    "\n",
    "language2sim = dict()\n",
    "\n",
    "for i in range(len(config.transformer)):\n",
    "    \n",
    "    topic_features = torch.load(f\"topic_features_{i}.pt\", map_location=torch.device(\"cuda\"))\n",
    "    content_features = torch.load(f\"content_features_{i}.pt\", map_location=torch.device(\"cuda\"))\n",
    "\n",
    "    print(f\"\\nCalculate Similiarity for: {config.transformer[i]}\")\n",
    "\n",
    "    for language in language_list:\n",
    "\n",
    "        language_content_index = content_language==language\n",
    "        language_topic_index = topic_language==language\n",
    "\n",
    "        language_content_ids = content_ids[language_content_index]\n",
    "        language_topic_ids = topic_ids[language_topic_index]\n",
    "\n",
    "        language_content_features = content_features[language_content_index]\n",
    "        language_topic_features = topic_features[language_topic_index] \n",
    "\n",
    "        if len(language_topic_features) > 0 and len(language_content_features) > 0:\n",
    "\n",
    "            if language_topic_features.dim() == 1:\n",
    "                language_topic_features = language_topic_features.unsqueeze(0)\n",
    "\n",
    "            if language_content_features.dim() == 1:\n",
    "                language_content_features = language_content_features.unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sim = language_topic_features @ language_content_features.T  \n",
    "\n",
    "            old_sim = language2sim.get(language, None)\n",
    "\n",
    "            if old_sim is None:\n",
    "                language2sim[language] = sim   \n",
    "            else:\n",
    "                language2sim[language] += sim\n",
    "\n",
    "                    \n",
    "del topic_features, content_features     \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "              \n",
    "\n",
    "num_models = len(config.transformer)\n",
    "    \n",
    "topic_list = []\n",
    "content_list = []\n",
    "\n",
    "print(f\"Calculate mean of similiarities of {num_models} models per language\")\n",
    "\n",
    "print(f\"\\nSelect content per language using dynamic threshold of {config.margin}:\")\n",
    "\n",
    "for language in language_list: \n",
    "    \n",
    "    language_content_ids = content_ids[content_language==language]\n",
    "    language_topic_ids = topic_ids[topic_language==language]\n",
    "\n",
    "    # Mean of Similiarities\n",
    "    sim_matrix = language2sim[language] \n",
    "    \n",
    "    #print(f\"Sim MIN: {sim_matrix.min():.3f} - Sim MAX: {sim_matrix.max():.3f} ->  mean for {num_models} models\")\n",
    "    \n",
    "    sim_matrix /= num_models\n",
    "    \n",
    "    #print(f\"Sim MIN: {sim_matrix.min():.3f} - Sim MAX: {sim_matrix.max():.3f}\")\n",
    "    \n",
    "    selection_length = []\n",
    "    \n",
    "    for i in range(len(sim_matrix)):\n",
    "        \n",
    "        topic = language_topic_ids[i]\n",
    "\n",
    "        sim = sim_matrix[i]\n",
    "\n",
    "        th_tmp = sim.max() - config.margin * sim.max()\n",
    "        p_select = (sim >= th_tmp).squeeze()\n",
    "        c_choice = set(language_content_ids[p_select.cpu().numpy()].tolist())\n",
    "        \n",
    "        topic_list.append(topic)\n",
    "        content_list.append(\" \".join(list(c_choice)))\n",
    "        selection_length.append(len(c_choice))\n",
    "\n",
    "    \n",
    "    selection_length = np.array(selection_length).mean()\n",
    "        \n",
    "    print(f\"{language.ljust(3)} - ({sim_matrix.shape[0]}x{sim_matrix.shape[1]}) - selected: {selection_length:.0f}\")    \n",
    "    \n",
    "del sim_matrix, language2sim\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9178ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:33:35.630942Z",
     "iopub.status.busy": "2023-03-14T23:33:35.630640Z",
     "iopub.status.idle": "2023-03-14T23:33:35.643418Z",
     "shell.execute_reply": "2023-03-14T23:33:35.642520Z"
    },
    "papermill": {
     "duration": 0.022445,
     "end_time": "2023-03-14T23:33:35.645683",
     "exception": false,
     "start_time": "2023-03-14T23:33:35.623238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({\"topic_id\": topic_list,\n",
    "                              \"content_ids\": content_list})\n",
    "    \n",
    "df_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05d9d2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:33:35.659469Z",
     "iopub.status.busy": "2023-03-14T23:33:35.659180Z",
     "iopub.status.idle": "2023-03-14T23:33:35.664655Z",
     "shell.execute_reply": "2023-03-14T23:33:35.663785Z"
    },
    "papermill": {
     "duration": 0.014771,
     "end_time": "2023-03-14T23:33:35.666785",
     "exception": false,
     "start_time": "2023-03-14T23:33:35.652014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.verbose:\n",
    "    display(df_submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "917b8bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:33:35.681014Z",
     "iopub.status.busy": "2023-03-14T23:33:35.680210Z",
     "iopub.status.idle": "2023-03-14T23:33:35.686845Z",
     "shell.execute_reply": "2023-03-14T23:33:35.685875Z"
    },
    "papermill": {
     "duration": 0.015862,
     "end_time": "2023-03-14T23:33:35.688871",
     "exception": false,
     "start_time": "2023-03-14T23:33:35.673009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f2_score(gt, pd):\n",
    "\n",
    "    gt = set(gt)\n",
    "    pd = set(pd)\n",
    "\n",
    "    if len(pd) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = len(gt.intersection(pd)) / len(pd)\n",
    "\n",
    "\n",
    "    if len(gt) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = len(gt.intersection(pd)) / len(gt)\n",
    "\n",
    "\n",
    "    if (4 * precision + recall) == 0.0:\n",
    "        f2 = 0.0\n",
    "    else:\n",
    "        f2 = (5 * precision * recall) / (4 * precision + recall)\n",
    "        \n",
    "    return f2, precision, recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da74f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T23:33:35.702811Z",
     "iopub.status.busy": "2023-03-14T23:33:35.702550Z",
     "iopub.status.idle": "2023-03-14T23:33:35.710791Z",
     "shell.execute_reply": "2023-03-14T23:33:35.709863Z"
    },
    "papermill": {
     "duration": 0.017788,
     "end_time": "2023-03-14T23:33:35.712913",
     "exception": false,
     "start_time": "2023-03-14T23:33:35.695125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.output_test:\n",
    "\n",
    "    df_correlations = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv\")\n",
    "\n",
    "    gt_dict = dict()\n",
    "\n",
    "    topics = df_correlations[\"topic_id\"].values\n",
    "    content = df_correlations[\"content_ids\"].values\n",
    "\n",
    "    for i in range(len(topics)):\n",
    "        content_tmp = content[i].split(\" \")\n",
    "        topic_tmp = topics[i]\n",
    "        gt_dict[topic_tmp] = content_tmp\n",
    "        \n",
    "\n",
    "    scores = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    " \n",
    "    for i, t in enumerate(topic_list):\n",
    "        \n",
    "        c = content_list[i].split(\" \")\n",
    "        \n",
    "        gt = gt_dict[t]\n",
    "\n",
    "        f, precision, recall = f2_score(gt, c)\n",
    "\n",
    "        scores.append(f)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "  \n",
    "    f2 = np.array(scores).mean() \n",
    "    precision = np.array(precision_list).mean()\n",
    "    recall = np.array(recall_list).mean()\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"Eval Score: {:.5f} - Precision: {:.5f} - Recall: {:.3f}\".format(f2, precision, recall))\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6574db3",
   "metadata": {
    "papermill": {
     "duration": 0.006108,
     "end_time": "2023-03-14T23:33:35.725236",
     "exception": false,
     "start_time": "2023-03-14T23:33:35.719128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 139.850855,
   "end_time": "2023-03-14T23:33:38.638606",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-14T23:31:18.787751",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
